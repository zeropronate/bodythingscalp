# Environment Variables for Blood Report Analyzer
# Copy this file to .env and adjust values as needed for your environment

# === OLLAMA LLM Configuration ===
# The LLM model to use (must be installed in Ollama)
OLLAMA_MODEL=llama3

# Ollama API endpoint (default: local machine on port 11434)
OLLAMA_API_URL=http://localhost:11434

# Maximum tokens to generate in LLM response (lower = faster)
OLLAMA_NUM_PREDICT=200

# Temperature for LLM response (0.0 = deterministic, 1.0 = random)
OLLAMA_TEMPERATURE=0.0

# === Timeout Configuration ===
# Base timeout for LLM calls in seconds (will be adjusted based on text length)
# Small texts (<1000 chars): 60s
# Medium texts (1000-2000 chars): 90s
# Large texts (>2000 chars): 120s
LLM_BASE_TIMEOUT_SECONDS=30

# Override timeout for specific requests (optional, in seconds)
# LLM_TIMEOUT_SECONDS=120

# === Text Processing ===
# Maximum characters to send to LLM (longer text gets truncated)
MAX_INPUT_CHARS=4000

# === Backend Configuration ===
# Backend API URL (used by frontend for API calls)
BACKEND_URL=http://localhost:8000

# === Optional: Frontend Port (if running Streamlit on different port) ===
# STREAMLIT_SERVER_PORT=8501

# === Optional: FastAPI Host ===
# FASTAPI_HOST=0.0.0.0
# FASTAPI_PORT=8000

